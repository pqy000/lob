{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@File    :   GLM-examples.py\\n@Time    :   2022/12/04\\n@Author  :   Aohan Zeng, Xiao Liu\\n@Contact :   zengaohan@gmail.com, shawliu9@gmail.com\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "@File    :   GLM-examples.py\n",
    "@Time    :   2022/12/04\n",
    "@Author  :   Aohan Zeng, Xiao Liu\n",
    "@Contact :   zengaohan@gmail.com, shawliu9@gmail.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "# We recommend you to use anaconda to config the virtual python environment\n",
    "!conda config --set channel_priority strict\n",
    "!conda install pytorch cudatoolkit -c conda-forge\n",
    "!pip install transformers==4.24.0 scipy==1.5.0 datasets==2.7.0 promptsource==0.2.3 sentencepiece scikit-learn tqdm jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For those students applying for GPUs:\n",
    "# If you want to run jupyter notebook remotely on the GPU server and open the editor on your laptop browser,\n",
    "# 1. (On remote server, under your conda environment): jupyter notebook --port 23041 (or any ports except for 8888)\n",
    "# 2. (On your laptop): ssh -p <provided_ssh_port> -L 8888:127.0.0.1:23041 <username>@221.194.152.90\n",
    "# 3. (On your laptop): open http://127.0.0.1:8888 on your laptop browser\n",
    "# If the token is required, find the token on the server console (e.g., http://127.0.0.1:23041/?token=<token>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f21961",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "example = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download the model and the tokenizer\n",
    "# DEFAULT: glm-2b, which is able for good zero-shot short blanking infilling ([MASK]) and long left-to-right generation ([gMASK])\n",
    "# If you want to do fine-tuning on language understanding or generation,\n",
    "# try smaller glm-roberta-large (335M, not for zero-shot)\n",
    "model_type = \"glm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, trust_remote_code=True, revision='main')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_type, trust_remote_code=True, revision='main').half().cuda()\n",
    "print(f\"Model {model_type} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For the generation task, we need to do conditional generation\n",
    "# Remember to refer to code example (https://github.com/THUDM/GLM#generation) in GLM's repo to find code for loss implementation!!!\n",
    "def generate_text(text, max_length=512):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = tokenizer.build_inputs_for_generation(inputs, max_gen_length=max_length)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "    # greedy decode strategy (topk = 1)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, eos_token_id=tokenizer.eop_token_id, top_k=1)[0].tolist()\n",
    "    sop_id = tokenizer.sop_token_id\n",
    "    eop_id = tokenizer.eop_token_id\n",
    "    end_idx = outputs.index(eop_id) if eop_id in outputs else len(outputs)\n",
    "    return tokenizer.decode(outputs[outputs.index(sop_id) + 1: end_idx]).strip()\n",
    "\n",
    "print(generate_text(\"Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample: tensor([-6.3198, -6.7733], device='cuda:0', grad_fn=<StackBackward0>)\n",
      "Batch samples: tensor([[-13.3635, -11.1453],\n",
      "        [ -4.7763,  -6.3134]], device='cuda:0', grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# For the classification task, in a Seq2Seq model like GLM, we need to calculate the conditional probability of choices for the given context.\n",
    "# Remember to refer to code example (https://github.com/THUDM/GLM#classification) in GLM's repo.\n",
    "\n",
    "# The `cond_log_prob` could be used for both multiple-choice problem (i.e., classification) or text generation (i.e., summurization).\n",
    "def cond_log_prob_single_sample(context, choices):\n",
    "    \"\"\"\n",
    "    Compute conditonal probability for one or more continuation/infilling options, single-sample only.\n",
    "    General solution to all classification/multiple-choice tasks.\n",
    "    :param context: prompted inputs. For example, \"One plus one equals two, is it correct? Answer: [MASK]\"\n",
    "    :param choices: classification labels or choices. For example, [\"No\", \"Yes\"]\n",
    "    \"\"\"\n",
    "    context_id = tokenizer(context)['input_ids']\n",
    "    probs = []\n",
    "    for choice in choices:\n",
    "        choice_id = tokenizer(' ' + choice)['input_ids'][1:-1]  # Feature of SentencePiece tokenizer\n",
    "        input_ids = torch.tensor(context_id + [tokenizer.sop_token_id] + choice_id[:-1], dtype=torch.long)\n",
    "        attention_mask = torch.tril(torch.ones(len(input_ids), len(input_ids), dtype=torch.long))\n",
    "        attention_mask[:len(context_id), :len(context_id)] = 1\n",
    "        mask_position = context_id.index(tokenizer.mask_token_id)\n",
    "        position_id = torch.cat([torch.arange(len(context_id)), torch.ones(len(choice_id)) * mask_position])\n",
    "        block_position_id = torch.cat([torch.zeros(len(context_id)), torch.arange(1, 1 + len(choice_id))])\n",
    "        position_id = torch.stack((position_id, block_position_id), dim=0).long()\n",
    "        logits = model.forward(input_ids=input_ids.view(1, -1).cuda(),\n",
    "                            attention_mask=attention_mask.unsqueeze(0).unsqueeze(0).cuda(),\n",
    "                            position_ids=position_id.view(1, 2, -1).cuda())['logits']\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "        probs.append(logits[0, range(len(context_id), len(context_id) + len(choice_id)), choice_id].sum())\n",
    "    return torch.stack(probs)\n",
    "\n",
    "print(\"Single sample:\", cond_log_prob_single_sample(\"One plus one equals two, is it correct? Answer: [MASK]\", [\"No\", \"Yes\"]))\n",
    "\n",
    "\n",
    "# Forward results by single sample is slow. The following codes organize a batch of inputs to speed up training.\n",
    "def build_multiple_choice_sample(context, choices):\n",
    "    context_id = tokenizer(context)['input_ids']\n",
    "\n",
    "    division = len(context_id)\n",
    "    mask_position = context_id.index(tokenizer.mask_token_id)\n",
    "\n",
    "    token = np.array(context_id, dtype=np.int64)\n",
    "    attention_mask = [np.ones((division, division), dtype=np.int64)]\n",
    "    position_id = np.arange(division, dtype=np.int64)\n",
    "    block_position_id = np.zeros(division, dtype=np.int64)\n",
    "\n",
    "    choice_target_id = []\n",
    "    choice_id = []\n",
    "\n",
    "    for choice_str in choices:\n",
    "        choice = np.array(tokenizer(choice_str)['input_ids'][1:-1], dtype=np.int64)\n",
    "\n",
    "        choice_id.append(choice)\n",
    "        choice_target_id.append(np.arange(len(token), len(token) + len(choice), dtype=np.int64))\n",
    "        attention_mask.append(np.tril(np.ones((len(choice), len(choice)), dtype=np.int64)))\n",
    "\n",
    "        token = np.concatenate((token, [tokenizer.sop_token_id], choice[:-1]))\n",
    "        position_id = np.concatenate((position_id, [mask_position] * len(choice)))\n",
    "        block_position_id = np.concatenate((block_position_id, np.arange(1, 1 + len(choice), dtype=np.int64)))\n",
    "\n",
    "    attention_mask = block_diag(*attention_mask)\n",
    "    attention_mask[division:, :division] = 1\n",
    "\n",
    "    return {\n",
    "        \"token\": token,\n",
    "        \"position_id\": np.stack((position_id, block_position_id)),\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"choices\": choice_id,\n",
    "        \"choice_target_ids\": choice_target_id\n",
    "    }\n",
    "\n",
    "\n",
    "def pad_batch(tokens, position_ids, attention_mask, max_seq_length):\n",
    "    pad_length = max_seq_length - len(tokens)\n",
    "    attention_mask = np.pad(\n",
    "        attention_mask,\n",
    "        pad_width=((0, pad_length),),\n",
    "        mode=\"constant\",\n",
    "        constant_values=0,\n",
    "    )\n",
    "    tokens = np.concatenate((tokens, np.zeros(pad_length, dtype=np.int64)))\n",
    "    position_ids = np.concatenate((position_ids, position_ids[..., -1:].repeat(pad_length, -1)), axis=-1)\n",
    "    return tokens, position_ids, attention_mask\n",
    "\n",
    "\n",
    "def collate_fn(samples):\n",
    "    TILE = 16\n",
    "    length_to_pad = (max(map(lambda spl: len(spl[\"token\"]), samples)) + TILE - 1) // TILE * TILE\n",
    "\n",
    "    token_batch, position_id_batch, attention_mask_batch = [], [], []\n",
    "    choices_batch, choice_target_ids_batch = [], []\n",
    "\n",
    "    for sample in samples:\n",
    "        token, position_id, attention_mask = pad_batch(\n",
    "            sample[\"token\"], sample[\"position_id\"], sample[\"attention_mask\"], length_to_pad\n",
    "        )\n",
    "        token_batch.append(token)\n",
    "        position_id_batch.append(position_id)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        choices_batch.append(sample[\"choices\"])\n",
    "        choice_target_ids_batch.append(sample[\"choice_target_ids\"])\n",
    "\n",
    "    return {\n",
    "        \"tokens\": torch.tensor(np.array(token_batch), dtype=torch.int64),\n",
    "        \"position_ids\": torch.tensor(np.array(position_id_batch), dtype=torch.int64),\n",
    "        \"attention_mask\": torch.tensor(np.array(attention_mask_batch), dtype=torch.int64),\n",
    "        \"choices\": choices_batch,\n",
    "        \"choice_target_ids\": choice_target_ids_batch,\n",
    "    }\n",
    "\n",
    "def cond_log_prob(context: List[str], choices: List[List[str]]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Compute conditonal probability for one or more continuation/infilling options.\n",
    "    :return The log probablity of each option.\n",
    "    \"\"\"\n",
    "    if not isinstance(context, list):\n",
    "        context = [context]\n",
    "        choices = [choices]\n",
    "    choices = [[(' ' + choice) for choice in choice_pair] for choice_pair in choices]  # Feature of SentencePiece tokenizer\n",
    "\n",
    "    samples = [build_multiple_choice_sample(ctx, ch) for ctx, ch in zip(context, choices)]\n",
    "\n",
    "    batch = collate_fn(samples)\n",
    "\n",
    "    logits = model.forward(input_ids=batch['tokens'].cuda(),\n",
    "                        attention_mask=batch['attention_mask'].cuda().unsqueeze(1),\n",
    "                        position_ids=batch['position_ids'].cuda())['logits']\n",
    "\n",
    "    log_probs = []\n",
    "\n",
    "    for output, choices, choice_target_ids in zip(F.log_softmax(logits, dim=-1), batch['choices'], batch['choice_target_ids']):\n",
    "        log_probs_single = []\n",
    "        for choice, choice_target_id in zip(choices, choice_target_ids):\n",
    "            tmp = output[choice_target_id, choice]\n",
    "            log_probs_single.append(tmp.sum())\n",
    "        log_probs.append(torch.stack(log_probs_single))\n",
    "\n",
    "    return torch.stack(log_probs)\n",
    "\n",
    "print(\"Batch samples:\", cond_log_prob([\"Tsinghua University is located in [MASK] .\",\n",
    "                                       \"One minus one equals zero, is it correct? Answer: [MASK]\"],\n",
    "                                      [[\"Beijing\", \"Shanghai\"],\n",
    "                                       [\"No\", \"Yes\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example: evaluating glm's zero-shot perfomance on glue/sst2 using prompt from promptsource\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "glue_sst2_prompts = DatasetTemplates('glue/sst2')  # WARNING: glue/super_glue/twitter_eval datasets are not allowed in your submission. This is only an example implementation.\n",
    "print(\"Prompt names:\", [prompt.get_name() for prompt in glue_sst2_prompts.templates.values()])\n",
    "# Remember to choose those prompts annotated as `original_task: true`; they are standard prompts.\n",
    "prompt = glue_sst2_prompts[\"review\"]\n",
    "choices = prompt.answer_choices.split(' ||| ')\n",
    "print(\"Choices:\", choices)\n",
    "\n",
    "correct = 0\n",
    "for sample in tqdm(dataset):\n",
    "    result = prompt.apply(sample)\n",
    "    context = result[0] + \"Answer: [MASK]\"\n",
    "    probs = cond_log_prob(context, choices)\n",
    "    pred = torch.argmax(probs).item()\n",
    "    correct += pred == sample['label']\n",
    "\n",
    "print(correct / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fine-tuning GLM on classification/mutliple-choice dataset. An example.\n",
    "import torch\n",
    "\n",
    "class MultipleChoiceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_name, split, prompt_name, tokenizer):\n",
    "        super(MultipleChoiceDataset, self).__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.prompt = DatasetTemplates(self.dataset_name)[prompt_name]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Ensure that the dataset split is valid.\n",
    "        self.data = []\n",
    "        if '/' in dataset_name:\n",
    "            iters = load_dataset(dataset_name.split('/')[0], dataset_name.split('/')[1], split=split)\n",
    "        else:\n",
    "            iters = load_dataset(dataset_name, split=split)\n",
    "        for sample in tqdm(iters):\n",
    "            self.data.append(dict(zip(\n",
    "                ['inputs_pretokenized', 'choices_pretokenized', 'label'],\n",
    "                self.prompting_single_sample(sample)\n",
    "            )))\n",
    "\n",
    "    def get_choices(self, sample):\n",
    "        \"\"\"\n",
    "        Default solution for text classification.\n",
    "        TODO: not applicable to multiple-choice problem. Please customize choices from `sample`.\n",
    "        \"\"\"\n",
    "        return self.prompt.answer_choices.split(' ||| ')\n",
    "\n",
    "    def prompting_single_sample(self, sample):\n",
    "        \"\"\"\n",
    "        Format a sample into a prompted sample.\n",
    "        :return inputs_pretokenized, choices_pretokenized\n",
    "        \"\"\"\n",
    "        inputs_pretokenized, groundtruth_choice = tuple(self.prompt.apply(sample))\n",
    "        choices_pretokenized = self.get_choices(sample)\n",
    "\n",
    "        # TODO: Use default label. Please customize according to your dataset.\n",
    "        label = sample['label']\n",
    "        return inputs_pretokenized + ' [MASK]', choices_pretokenized, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "class ConditionalGenerationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    TODO: implement your generation task dataset.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement your trainer based on the provided functions and dataset.\n",
    "# Consider using pytorch_lightening or Huggingface to setup your trainer and training arguments\n",
    "# Remember to set model.float() and model.train() before fine-tuning, since fp16 training is instable without deepspeed.\n",
    "# Acknowledgement: Code in this cell is adapted from Hongzun Liu's submitted homework and implementation. TA Xiao Liu made the adaptations.\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def init_logger():\n",
    "    logger = logging.getLogger(\"default\")\n",
    "    cmd_handler = logging.StreamHandler(sys.stdout)\n",
    "    cmd_handler.setLevel(logging.DEBUG)\n",
    "    cmd_handler.setFormatter(logging.Formatter(r\"[%(asctime)s][%(levelname)s][%(filename)s:%(lineno)s] %(message)s\"))\n",
    "    logger.addHandler(cmd_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "\n",
    "\n",
    "# # Clean previous loaded model (2B is too large for fine-tuning in our HW)\n",
    "# del model, tokenizer\n",
    "\n",
    "# Reload smaller `glm-roberta-large` model and tokenizer\n",
    "model_type = \"BAAI/glm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, trust_remote_code=True, revision='main')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_type, trust_remote_code=True, revision='main').cuda()\n",
    "\n",
    "\n",
    "def flatten_labels(compacted_labels):\n",
    "    batch_size = len(compacted_labels[0])\n",
    "    num_of_classes = len(compacted_labels)\n",
    "    return [[compacted_labels[i][idx] for i in range(num_of_classes)] for idx in range(batch_size)]\n",
    "\n",
    "\n",
    "# An example trainer for multiple-choice classification.\n",
    "class MultipleChoiceTrainer:\n",
    "    def __init__(self, dataset_name: str, prompt_name: str):\n",
    "        self.train_bsz, self.eval_bsz = 8, 8\n",
    "        self.epoch = 10\n",
    "        self.lr = 1e-5\n",
    "        # Load tokenizer & logger\n",
    "        self.tokenizer = tokenizer  # use tokenizer from 3rd cell\n",
    "        self.logger = init_logger()\n",
    "\n",
    "        # Load dataset\n",
    "        self.train_dataset = MultipleChoiceDataset(dataset_name, 'train', prompt_name, self.tokenizer)\n",
    "        self.valid_dataset = MultipleChoiceDataset(dataset_name, 'validation', prompt_name, self.tokenizer)\n",
    "        self.test_dataset = MultipleChoiceDataset(dataset_name, 'test', prompt_name, self.tokenizer)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.train_bsz, shuffle=True, drop_last=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=self.eval_bsz, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.eval_bsz, shuffle=False)\n",
    "\n",
    "        # Configure training model, optimizer, and scheduler\n",
    "        self.model = model.float()  # use model from 3rd cell\n",
    "        self.model.train()\n",
    "        num_training_steps = self.epoch * (len(self.train_dataset) // self.train_bsz)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n",
    "                                                         num_warmup_steps=int(num_training_steps * 0.06),\n",
    "                                                         num_training_steps=num_training_steps)\n",
    "\n",
    "    def evaluate(self, e, data_loader):\n",
    "        valid_loss = 0.0\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in tqdm(enumerate(data_loader, start=1), desc=\"valid\", total=len(data_loader)):\n",
    "                logits = cond_log_prob(sample[\"inputs_pretokenized\"], flatten_labels(sample['choices_pretokenized']))\n",
    "                labels = sample[\"label\"].cuda()\n",
    "                loss = F.nll_loss(logits, labels)\n",
    "                valid_loss += loss.item()\n",
    "                valid_preds.extend(torch.argmax(logits, dim=-1).cpu().numpy().tolist())\n",
    "                valid_labels.extend(np.array(sample[\"label\"]).tolist())\n",
    "        valid_loss = valid_loss / len(data_loader)\n",
    "        valid_acc = accuracy_score(valid_preds, valid_labels)\n",
    "        self.logger.info(f\"[VALID] epoch {e}: loss={valid_loss}, acc={valid_acc}\")\n",
    "\n",
    "    def train(self):\n",
    "        for e in range(1, self.epoch + 1):\n",
    "            self.logger.info(f\"Epoch {e}\")\n",
    "            # train\n",
    "            tqdm_vars = {\"lr\": np.nan, \"loss\": np.nan}\n",
    "            tbar = tqdm(enumerate(self.train_loader, start=1), desc=\"train\", total=len(self.train_loader),\n",
    "                        postfix=tqdm_vars)\n",
    "            train_loss_value = 0.0\n",
    "            model.train()\n",
    "            for i, sample in tbar:\n",
    "                logits = cond_log_prob(sample[\"inputs_pretokenized\"], flatten_labels(sample['choices_pretokenized']))\n",
    "                labels = sample[\"label\"].cuda()\n",
    "                loss = F.nll_loss(logits, labels)\n",
    "                train_loss_value += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                tqdm_vars[\"lr\"] = self.optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "                tqdm_vars[\"loss\"] = train_loss_value\n",
    "                tbar.set_postfix(tqdm_vars)\n",
    "                train_loss_value = 0.0\n",
    "            # valid\n",
    "            self.evaluate(e, self.valid_loader)\n",
    "        # TODO: If there is a test dataset, please select the best-performed checkpoints on valid dataset to evaluate.\n",
    "        # TODO: the example `glue/rte` has no public test set.\n",
    "\n",
    "\n",
    "# Don't use datasets from super_glue/glue/twitter_eval in your submitted HW!\n",
    "trainer = MultipleChoiceTrainer(dataset_name=\"glue/rte\", prompt_name=\"mean\")  # Choose a `original_task: true` prompt!\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}